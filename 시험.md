## chap 2 (보통 문제 1번)
### 데이터 구조 및 전처리
```
Train_images.shape
Len(train_labels)  -> 두개 길이 일치 찾는거
Train_labels -> 구조 확인
```

### 전처리
```
train_images.reshape((60000, 28*28)) -> 28*28의 크기가 되도록
train_images.astype("float")/255 -> 0 ~ 1 범위가 되도록
```

### 간단한 sequential model + 결과창 확인
```
from tensorflow import keras
from tensorflow.keras import layers
model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(10, activation="softmax")
])
출제 포인트 -> activation 고르기? multi에는 softmax / binomial에는 sigmoid
 
만약 전처리 안해도 되는 형태라면? 
model = models.Sequential([
    layers.Dense(3, activation = "relu"),
    layers.Dense(1, activation = "sigmoid")
])
model.build(input_shape=(None, 2)) -> 여기서 2는 input.shape에서 확인 가능(1000,2)
model.summary()
```

### 컴파일
```
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
optimizer, loss, metrics 뭐로 하라고 지정 (적절한 loss 설정하라는 거)
-> sparse_categorical_crossentropy (분류가 여러개의 정수값 + softmax)
-> 	binary_crossentropy   (이진 분류 + sigmoid)
-> categorical_crossentropy (softmax + 분류가 원핫인코딩)
-> mean_squared_error <- 회귀 문제
// 
```

### 모델 돌리기

```
Fit your model using 8 number of epochs with batch size 50. Specify validation data as (test_images, test, labels)
history = model.fit(  # Code here
                    train_images,
                    train_labels,
                    epochs=8,
                    batch_size=50,
                    validation_data=(test_images, test_labels))
+ 평가
## Evaluate your model. Your code here:
model.evaluate( test_images, test_labels)
```

----------------------------------------------------------------------------------------------------

## 시각화 (분류)
```
## Plot of the ground truth set
import matplotlib.pyplot as plt
plt.scatter(inputs_dev[:, 0], inputs_dev[:, 1], c=targets_dev)
plt.show()
-> 원래 데이터

import matplotlib.pyplot as plt
res1 = model.predict(inputs_dev)
decision1 = res1 > .5
plt.scatter(inputs_dev[:, 0], inputs_dev[:, 1], c=decision1)
plt.show()

-> 내 모델로 평가
```

## dense_layer 수동
```
from tensorflow import keras

class SimpleDense(keras.layers.Layer):

    def __init__(self, units, activation=None):
        super().__init__()
        self.units = units
        self.activation = activation

    def build(self, input_shape):
        input_dim = input_shape[-1]
        self.W = self.add_weight(shape=(input_dim, self.units),
                                 initializer="random_normal")
        self.b = self.add_weight(shape=(self.units,),
                                 initializer="zeros")

    def call(self, inputs):
        y = tf.matmul(inputs, self.W) + self.b
        if self.activation is not None:
            y = self.activation(y)
        return y

--
my_dense = SimpleDense(units=32, activation=tf.nn.relu)
input_tensor = tf.ones(shape=(2, 784))
output_tensor = my_dense(input_tensor)
print(output_tensor.shape)
```

-------------------------------------------------------------------

## vectorize
```
- 정수형 아닐 때, 인덱스에 넣어서 찾아줌
def vectorize_sequences(sequences, dimension):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        for j in sequence:
            try :
              ### Modify this part
              results[i, word_index[j]] = 1.
            except :
              pass
    return results

x4train = vectorize_sequences(X_train, dimension= len(sorted_words))
x4val = vectorize_sequences(X_val, dimension= len(sorted_words))
x4test = vectorize_sequences(X_test, dimension= len(sorted_words))

정수형일때, 그냥 넣음
import numpy as np
def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        for j in sequence:
            results[i, j] = 1.
    return results
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
```

```
원핫인코딩

def to_one_hot(labels, dimension=46):
    results = np.zeros((len(labels), dimension))
    for i, label in enumerate(labels):
        results[i, label] = 1.
    return results
y_train = to_one_hot(train_labels)
y_test = to_one_hot(test_labels)

y_train = to_categorical(train_labels)
y_test = to_categorical(test_labels)
```

## 셔플에서 앞 거 뽑기
```
import numpy as np

indices_permutation = np.random.permutation(404)
shuffled_x_train = train_data[indices_permutation]
shuffled_train_targets = train_targets[indices_permutation]

num_validation_samples = 54

## Complete the code below:
val_x = shuffled_x_train[:54] ## take first 54 samples of shuffled_x_train
val_targets = shuffled_train_targets[:54] ## take first 54 samples of shuffled_train_targets


trn_x = shuffled_x_train[num_validation_samples:]
trn_targets = shuffled_train_targets[num_validation_samples:]
```

## 시각화(mae, mse 등)
```
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
```

```
history_dict = history.history
history_dict.keys()
```

```
import matplotlib.pyplot as plt
history_dict = history.history
loss_values = history_dict["loss"]
val_loss_values = history_dict["val_loss"]
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()
```

```
import matplotlib.pyplot as plt
history_dict = history.history
epochs = range(1, len(loss_values) + 1)
plt.clf()
acc = history_dict["accuracy"]
val_acc = history_dict["val_accuracy"]
plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

epoch별 acc
```


## 5chap- noise/ zero channel
```
from tensorflow.keras.datasets import mnist
import numpy as np

(train_images, train_labels), _ = mnist.load_data()
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255

train_images_with_noise_channels = np.concatenate(
    [train_images, np.random.random((len(train_images), 784))], axis=1)

train_images_with_zeros_channels = np.concatenate(
    [train_images, np.zeros((len(train_images), 784))], axis=1)
```

## def 활용해서 편하게 두번 학습
```
from tensorflow import keras
from tensorflow.keras import layers

def get_model():
    model = keras.Sequential([
        layers.Dense(512, activation="relu"),
        layers.Dense(10, activation="softmax")
    ])
    model.compile(optimizer="rmsprop",
                  loss="sparse_categorical_crossentropy",
                  metrics=["accuracy"])
    return model

model = get_model()
history_noise = model.fit(
    train_images_with_noise_channels, train_labels,
    epochs=10,
    batch_size=128,
    validation_split=0.2)

model = get_model()
history_zeros = model.fit(
    train_images_with_zeros_channels, train_labels,
    epochs=10,
    batch_size=128,
    validation_split=0.2)
```

랜덤한 라벨로 학습 - 
```
(train_images, train_labels), _ = mnist.load_data()
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255

random_train_labels = train_labels[:]
np.random.shuffle(random_train_labels)

model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(10, activation="softmax")
])
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
model.fit(train_images, random_train_labels,
          epochs=100,
          batch_size=128,
          validation_split=0.2)
```

## rmsprop(1.0) -> 너무 큰 learning late -> 수업때는 rmsprop(1e-2)으로 함
```
(train_images, train_labels), _ = mnist.load_data()
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255

model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(10, activation="softmax")
])
model.compile(optimizer=keras.optimizers.RMSprop(1.),
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
model.fit(train_images, train_labels,
          epochs=10,
          batch_size=128,
          validation_split=0.2)
```

## 로지스틱 회귀 -> layer softmax 하나인 
```
model = keras.Sequential([layers.Dense(10, activation="softmax")])
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
history_small_model = model.fit(
    train_images, train_labels,
    epochs=20,
    batch_size=128,
    validation_split=0.2)
```
```
시각
model = keras.Sequential([layers.Dense(10, activation="softmax")])
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
history_small_model = model.fit(
    train_images, train_labels,
    epochs=20,
    batch_size=128,
    validation_split=0.2)
```

## capaxity -> layer에서 조절 가능
```
model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(512, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
history_larger_model = model.fit(
    train_data, train_labels,
    epochs=20, batch_size=512, validation_split=0.4)
512 -> 과적합 16 -> 오리지널 4 -> 과소적
```

#L2 정규화 -> 웨이트 너무 과하게 하는거 방지
```
from tensorflow.keras import regularizers
model = keras.Sequential([
    layers.Dense(16,
                 kernel_regularizer=regularizers.l2(0.002),
                 activation="relu"),
    layers.Dense(16,
                 kernel_regularizer=regularizers.l2(0.002),
                 activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
history_l2_reg = model.fit(
    train_data, train_labels,
    epochs=20, batch_size=512, validation_split=0.4)

from tensorflow.keras import regularizers
regularizers.l1(0.001)
regularizers.l1_l2(l1=0.001, l2=0.001)
이런 다른 정규화도 가능
```

## 드랍아웃
```
model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(16, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
history_dropout = model.fit(
    train_data, train_labels,
    epochs=20, batch_size=512, validation_split=0.4)
```

## 7챕- sequential

```
1. 기존 방법
model = keras.Sequential([
    layers.Dense(64, activation="relu", input_shape=(3,)),
    layers.Dense(10, activation="softmax")
])

2.add 활용 후 빌드에 인풋
model = keras.Sequential()
model.add(layers.Dense(64, activation="relu"))
model.add(layers.Dense(10, activation="softmax"))
model.build(input_shape=(None,3))
이러면 진짜 둘이 똑같다

웨이트 출력
model.weights
model.summary

3. 혹은 인풋 레이어도 추가 가능
model = keras.Sequential()
model.add(keras.Input(shape=(3,)))
model.add(layers.Dense(64, activation="relu"))
model.add(layers.Dense(10, activation="softmax", name="my_last_layer"))
```
